rf.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 3, ntree = 500, importance = TRUE)
yhat.rf <- predict(rf.carseats, newdata = Carseats.test)
mean((yhat.rf - Carseats.test$Sales)^2)
# important predictors
importance(rf.carseats)
# Chunk 12
str(sub.d.bike_weather)
sub.d.bike_weather$start_station_id <- as.numeric(sub.d.bike_weather$start_station_id)
# Chunk 13
str(sub.d.bike_weather)
lm.fit <- lm(log(tripduration) ~ weekday + public_holiday
, data=sub.d.bike_weather)
plot(lm.fit)
lm.fit <- lm(log(tripduration) ~ age, data=sub.d.bike_weather)
summ <- summary(lm.fit)
modcoef <- summ[["coefficients"]]
summ
modcoef
modcoef[ , 4] <- round(modcoef[ , 4], 3)
modcoef[order(modcoef[0:20 , 4]), ]
# save the model to disk
saveRDS(lm.fit, "./data/lm_model_log.rds")
# load the model
#read_model <- readRDS("./data/lm_model.rds")
#read_summ <- summary(read_model)
# Chunk 14
colnames(d.bike_weather)
# tripduration
hist(d.bike_weather$tripduration)
hist(log(d.bike_weather$tripduration))
# station id
hist(as.numeric(d.bike_weather$start_station_id))
hist(as.numeric(d.bike_weather$end_station_id))
# birth_year
describe(d.bike_weather$birth_year)
hist(as.numeric(d.bike_weather$age_capped))
# weather
hist(d.bike_weather$maximum_temperature_celsius)
hist(d.bike_weather$minimum_temperature_celsius)
hist(d.bike_weather$average_temperature_celsius)
describe(d.bike_weather$precipitation)
describe(d.bike_weather$snow_fall)
describe(d.bike_weather$snow_depth)
typeof(d.bike_weather$snow_depth)
# Chunk 15: corrplot
# round(cor(dplyr::select_if(d.bike_weather, is.numeric)),2)
rquery.cormat<-function(x,
type=c('lower', 'upper', 'full', 'flatten'),
graph=TRUE,
graphType=c("correlogram", "heatmap"),
col=NULL, ...)
{
library(corrplot)
# Helper functions
#+++++++++++++++++
# Compute the matrix of correlation p-values
cor.pmat <- function(x, ...) {
mat <- as.matrix(x)
n <- ncol(mat)
p.mat<- matrix(NA, n, n)
diag(p.mat) <- 0
for (i in 1:(n - 1)) {
for (j in (i + 1):n) {
tmp <- cor.test(mat[, i], mat[, j], ...)
p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
}
}
colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
p.mat
}
# Get lower triangle of the matrix
getLower.tri<-function(mat){
upper<-mat
upper[upper.tri(mat)]<-""
mat<-as.data.frame(upper)
mat
}
# Get upper triangle of the matrix
getUpper.tri<-function(mat){
lt<-mat
lt[lower.tri(mat)]<-""
mat<-as.data.frame(lt)
mat
}
# Get flatten matrix
flattenCorrMatrix <- function(cormat, pmat) {
ut <- upper.tri(cormat)
data.frame(
row = rownames(cormat)[row(cormat)[ut]],
column = rownames(cormat)[col(cormat)[ut]],
cor  =(cormat)[ut],
p = pmat[ut]
)
}
# Define color
if (is.null(col)) {
col <- colorRampPalette(
c("#67001F", "#B2182B", "#D6604D", "#F4A582",
"#FDDBC7", "#FFFFFF", "#D1E5F0", "#92C5DE",
"#4393C3", "#2166AC", "#053061"))(200)
col<-rev(col)
}
# Correlation matrix
cormat<-signif(cor(x, use = "complete.obs", ...),2)
pmat<-signif(cor.pmat(x, ...),2)
# Reorder correlation matrix
ord<-corrMatOrder(cormat, order="hclust")
cormat<-cormat[ord, ord]
pmat<-pmat[ord, ord]
# Replace correlation coeff by symbols
sym<-symnum(cormat, abbr.colnames=FALSE)
# Correlogram
if(graph & graphType[1]=="correlogram"){
corrplot(cormat, type=ifelse(type[1]=="flatten", "lower", type[1]),
tl.col="black", tl.srt=45,col=col,...)
}
else if(graphType[1]=="heatmap")
heatmap(cormat, col=col, symm=TRUE)
# Get lower/upper triangle
if(type[1]=="lower"){
cormat<-getLower.tri(cormat)
pmat<-getLower.tri(pmat)
}
else if(type[1]=="upper"){
cormat<-getUpper.tri(cormat)
pmat<-getUpper.tri(pmat)
sym=t(sym)
}
else if(type[1]=="flatten"){
cormat<-flattenCorrMatrix(cormat, pmat)
pmat=NULL
sym=NULL
}
list(r=cormat, p=pmat, sym=sym)
}
rquery.cormat(dplyr::select_if(d.bike_weather, is.numeric))
# Chunk 16
# average_temperature_celsius
# presipiation
colnames(d.bike_weather)
ggplot(data = d.bike_weather,
aes(
y = log(tripduration),
x = age,
color=gender))+
geom_point(alpha=0.1) +
geom_smooth(method='lm', formula= y~x)
# Chunk 17
# Linear Model Function
linear_model <- function(df, formula, str_splitDate){
# Train / Test split
splitDate <- as.POSIXct(str_splitDate)
train <- df[date < splitDate]
test <- df[date >= splitDate]
test.turnover <- test[,turnover]
test[,turnover := NULL]
range(train$date)
range(test$date)
# Modell fit
lm.mod1 <- lm(formula, data = train)
print(summary(lm.mod1))
plot(lm.mod1)
# Predict
lm.mod1.pred <- predict(lm.mod1, newdata = test)
plot(lm.mod1.pred)
lm.mod1.res <- lm.mod1.pred - test.turnover
ape <- abs(lm.mod1.res) / test.turnover
mape <- median(ape)
print(paste0("Formula: ", formula))
print(paste0("Median APE: ", mape))
# Plot
plotdata <- data.table(pred = lm.mod1.pred, turnover = test.turnover, res = lm.mod1.res, ape = ape, date = test$date)
predPlot <- ggplot(plotdata, aes(as.Date(date))) +
geom_line(aes(y = turnover, colour = "Turnover")) +
geom_line(aes(y = pred, colour = "Prediction")) +
theme_bw() +
scale_x_date(date_breaks = "1 week", date_labels =  "%d.%m.%Y") +
theme(axis.text.x = element_text(angle = 60, hjust = 1))
apePlot <- ggplot(plotdata, aes(as.Date(date))) +
geom_line(aes(y = ape, colour = "Abweichung [%]")) +
theme_bw() +
scale_x_date(date_breaks = "1 week", date_labels =  "%d.%m.%Y") +
theme(axis.text.x = element_text(angle = 60, hjust = 1))
grid.arrange(predPlot, apePlot, nrow = 1)
print(plotdata[order(-abs(res))])
}
# get background map
min_lon <- min(d.bike_weather$start_station_longitude, d.bike_weather$end_station_longitude)
max_lon <- max(d.bike_weather$start_station_longitude, d.bike_weather$end_station_longitude)
min_lat <- min(d.bike_weather$start_station_latitude, d.bike_weather$end_station_latitude)
max_lat <- max(d.bike_weather$start_station_latitude, d.bike_weather$end_station_latitude)
mad_map <- (map <- get_map(c(left = min_lon, bottom = min_lat,
right = max_lon, top = max_lat)))
# take a random sample of size 1%
d.bike_weather_sample = d.bike_weather[
base::sample(x=1:nrow(d.bike_weather), round(nrow(d.bike_weather)*1/100)), ]
colnames(d.bike_weather)
install.packages("ggpubr")
# Chunk 1: setup
# remove all objects loaded and clear memory
rm(list = ls(all.names = TRUE))
gc()
library(checkpoint)
#checkpoint(snapshotDate = "2099-12-29")
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
knitr::opts_chunk$set(echo=TRUE)
set.seed(19)
# Chunk 2: importLibraries
list.of.packages <- c("installr", "Hmisc", "ggmap", "tidyverse", "tidyr", "corrplot",
"viridis", "leaflet", "lubridate", "checkpoint", "zoo", "caTools",
"randomForest", "boot", "ggpubr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
for (p in list.of.packages){
library(p, character.only = TRUE)
}
# Chunk 3: user-functions
## user defined functions and constants ##
## mapping of 33 different weather conditions to 6 different weather types
sunny  <- c("Fair", "Fair / Windy")
cloudy <- c("Cloudy", "Mostly Cloudy", "Partly Cloudy", "Mostly Cloudy / Windy", "Haze")
rainy  <- c("Light Rain", "Light Drizzle", "Rain", "Heavy Rain", "Light Rain / Windy",
"Fog", "Light Freezing Drizzle", "Rain / Windy", "Heavy Rain / Windy",
"Haze / Windy", "Light Drizzle / Windy", "T-Storm", "Drizzle and Fog",
"Thunder", "Heavy T-Storm", "Light Rain with Thunder", "Patches of Fog")
windy  <- c("Partly Cloudy / Windy", "Cloudy / Windy", "Haze / Windy")
snowy  <- c("Light Snow", "Wintry Mix", "Snow", "Heavy Snow", "Light Snow / Windy",
"Snow / Windy", "Heavy Snow / Windy")
## holidays in New York City in 2016
public_holidays <- c("2016-01-01", "2016-01-18", "2016-02-12", "2016-02-15",
"2016-05-30", "2016-07-04", "2016-09-05", "2016-10-10",
"2016-11-11", "2016-11-24", "2016-12-26")
## convert temperature degree from Fahrenheit to Celsius
## @param fahrenheit: degree in fahrenheit
## @return degree in celcius
farenheit_to_celsius <- function(fahrenheit){
return(as.numeric(5/9*(fahrenheit-32)))
}
## convert miles per hour to kilometer per hour
## @param mph: miles per hour
## @return kilometer per hour
mph_to_kmh <- function(mph) {
return (as.numeric(1.61 * mph))
}
## convert 32 different weather conditions to 5 typical weather types
## @param conditions: a vector of weather condition values
## @return a vector of weather types corresponding to the weather conditions
get_weather_type_vector <- function (conditions) {
types <-
ifelse(conditions %in% sunny,"sunny",
ifelse(conditions %in% cloudy,"cloudy",
ifelse(conditions %in% rainy,"rainy",
ifelse(conditions %in% windy,"windy",
ifelse(conditions %in% snowy,"snowy", NA)))))
return (types)
}
## plot bike rental count over 24 hours for different category groups
## @param d.data: expects properties: category, hour, rental
## @param title: the title of the plot
## @param category_name: the category group name used as the legend's name in the plot
show_24h_category_statistics_plot <- function (d.data, title, category_name) {
ggplot(d.data, aes(x=hour, y=rental, color=category)) +
geom_point(data=d.data, aes(group=category)) +
geom_line(data=d.data, aes(group=category)) +
ggtitle(title) +
scale_color_hue(category_name, breaks=levels(d.data$category))
}
## process bike sharing data ##
d.bike.raw = read_csv("./data/NYC-CitiBike-2016.csv")
## replace white space by underscore in column names
d.bike.raw <- d.bike.raw %>% select_all(snakecase::to_snake_case)
## convert data
d.bike <- d.bike.raw %>%
select(starttime, tripduration) %>%
mutate(starttime = mdy_hms(starttime),
date = date(starttime), # YYYY-MM-DD
month = factor(month(starttime)),
weekday = factor(weekdays(starttime)),
hour = factor(hour(starttime)),
tripduration = round(tripduration/60) #second to minute
)
## This plot shows number of trips in minute durations grouped in hour durations
## The plot shows trips longer than 24 hours are less than 5 rentals
data.frame(duration = floor(d.bike$tripduration / 60)) %>%
group_by(duration) %>% summarise(count = n()) %>% filter(count > 5) %>% plot()
## we consider only trips with duration less than 24 hours
d.bike <- filter(d.bike, tripduration < 1441) # only use data with rental < 24 hours
## aggreate bike sharing data on hour basis, and only relevant predictos are
## used for the analysis
d.bike <- d.bike %>%
group_by(date, month, weekday, hour) %>%
summarise(tripduration=sum(tripduration), rental_count=n())
head(d.bike)
# Chunk 1: setup
# remove all objects loaded and clear memory
rm(list = ls(all.names = TRUE))
gc()
library(checkpoint)
#checkpoint(snapshotDate = "2099-12-29")
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
knitr::opts_chunk$set(echo=TRUE)
set.seed(19)
# Chunk 2: importLibraries
list.of.packages <- c("installr", "Hmisc", "ggmap", "tidyverse", "tidyr", "corrplot",
"viridis", "leaflet", "lubridate", "checkpoint", "zoo", "caTools",
"randomForest", "boot", "ggpubr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
for (p in list.of.packages){
library(p, character.only = TRUE)
}
# Chunk 3: userFunctions
## user defined functions and constants ##
## holidays in New York City in 2016
public_holidays <- c("2016-01-01", "2016-01-18", "2016-02-12", "2016-02-15",
"2016-05-30", "2016-07-04", "2016-09-05", "2016-10-10",
"2016-11-11", "2016-11-24", "2016-12-26")
## plot bike rental count over 24 hours for different category groups
## @param d.data: expects properties: category, hour, rental
## @param title: the title of the plot
## @param category_name: the category group name used as the legend's name in the plot
show_24h_category_statistics_plot <- function (d.data, title, category_name) {
ggplot(d.data, aes(x=hour, y=rental, color=category)) +
geom_point(data=d.data, aes(group=category)) +
geom_line(data=d.data, aes(group=category)) +
ggtitle(title) + ylab("rentals / hour") +
scale_color_hue(category_name, breaks=levels(d.data$category))
}
# Chunk 4
## read bike and weather data sets
d.bike <- readRDS("./data/d.bike.rds")
colnames(d.bike)
dim(d.bike)
d.weather <- readRDS("./data/d.weather.rds")
colnames(d.weather)
dim(d.weather)
## merge both bike and weather data for analysis
d.total <- merge(d.bike, d.weather, all.x=TRUE)
# order by data and hour
d.total <- d.total[with(d.total, order(date, hour)),]
d.total$weekday <- factor(d.total$weekday,
levels = c("Monday", "Tuesday", "Wednesday",
"Thursday", "Friday", "Saturday", "Sunday"))
# reset row index to normal
rownames(d.total) <- NULL
# Chunk 5
## public holiday analysis ##
## rental statistics in weekdays
rentweekday <- d.total %>%
group_by(weekday) %>%
summarise(rental=round(sum(rental_count)/52),
tripduration=round(sum(tripduration)/52)) %>% # 52 weeks of a year
arrange(weekday)
rentweekday
## rentals statistics in public holidays
rentholiday <- d.total %>%
filter(as.character(date) %in% public_holidays) %>%
group_by(date) %>%
summarise(rental=sum(rental_count),
tripduration=sum(tripduration)) %>%
arrange(date)
rentholiday
## As we can see, the rental in public holidays are different among and the weekdays.
## Public holiday data can be used for the model fitting if we would like to predict
## for a different year.
## However, only data from 2016 are available for both the analysis and model testing.
## Droping the public holiday data would reduce the variance in the model.
## remove public holiday dataset
d.total <- filter(d.total, !as.character(date) %in% public_holidays)
# Chunk 6
## save processed data to file
saveRDS(d.total, file = "./data/d.total.rds")
# Chunk 7
# ggplot 24h by weekday
d.weekday <- d.total %>% group_by(weekday, hour) %>%
summarise(rental = round(mean(rental_count))) %>%
rename(category = weekday)
show_24h_category_statistics_plot(d.weekday, "24h Rental by Weekday", "Weekday")
# Chunk 8
ggplot(d.total, aes(x=month, y=rental_count, color=month)) +
geom_boxplot(data=d.total, aes(group=month)) +
ggtitle("Hourly rental distribution on months") + ylab("rentals / hour")
# Chunk 9
# ggplot 24h by weather types
d.wtype <- d.total %>% group_by(type, hour) %>%
summarise(rental = round(mean(rental_count))) %>%
rename(category = type)
show_24h_category_statistics_plot(d.wtype, "24h Rental by Weather Type", "Weather Type")
# Chunk 10
## the rental of the weekdays with an influence of the weather
ggplot(d.total, aes(x=weekday, y=rental_count)) +
geom_point() +
geom_smooth(method="lm") +
ylab("rentals / hour") +
facet_grid(. ~ type) +
ggpubr::rotate_x_text()
# Chunk 11
## correlation plot
d.total[,5:10] %>% cor() %>% corrplot(method = 'color', addCoef.col="black")
# Chunk 12
## split a train and a test set
set.seed(1)
d.split <- sample.split(d.total, SplitRatio = 0.7)
d.train <- subset(d.total, subset = d.split)
d.test <- subset(d.total, subset = !d.split)
# Poisson model with all possible predictors
pm.fit <- glm(rental_count ~ . - rental_count - tripduration - date,
family="poisson", data=d.train)
#pm.err <- cv.glm(d.train, pm.fit, K=10)$delta[1]
#cat("10 K cross-validation error rate on Poisson model: ", pm.err)
# predict model on test data
pm.pred <- predict(pm.fit, newdata = d.test, type="response")
# compute MSE
pm.mse <- mean((d.test$rental_count - pm.pred)^2)
cat("Complex Poisson model MSE: ", pm.mse)
# Chunk 13
# residual
resid_values <- resid(pm.fit)
cat("Residual legnth: ", length(resid_values))
cat("Residual header: ", head(resid_values))
# residual analysis plots
par(mfrow=c(2,2))
plot(pm.fit)
# Chunk 14
# ggplot predicted values of hourly rental for different weather types
p1 <- ggplot(d.test, aes(x=hour, y=pm.pred, color=type)) +
geom_point(data=d.test, aes(group=type)) +
ggtitle("Prediction values") + ylab("predicted rentals / hour") +
scale_color_hue("Weather Type", breaks=levels(d.test$type))
# ggplot true values of hourly rental for different weather types
p2 <- ggplot(d.test, aes(x=hour, y=rental_count, color=type)) +
geom_point(data=d.test, aes(group=type)) +
ggtitle("Real values") + ylab("rentals / hour") +
scale_color_hue("Weather Type", breaks=levels(d.test$type))
figure <- ggarrange(p1, p2, ncol=2, common.legend = TRUE, legend="bottom")
annotate_figure(figure, top=text_grob("Hour & Weather on Rental - Complex Poisson Model",
size = 14))
# Chunk 15
## check normal distribution for linear model fitting
par(mfrow=c(1, 2))
hist(d.total$rental_count, main = "Rental Count Histogram")
hist(log(d.total$rental_count), main = "Rental Count Log Histogram")
# Chunk 16
set.seed(1)
cv.error.10 <- rep(0, 10)
for (i in 1:10) {
glm.fit <- glm(log(rental_count) ~ type + month + weekday + hour + humidity * dewpoint +
poly(windspeed, degree=i) + poly(temperature, degree=i), data=d.train)
cv.error.10[i] <- cv.glm(d.train, glm.fit, K=10)$delta[1]
}
best.level <- which.min(cv.error.10)
cat("Index of model with least error rate: ", best.level)
cat("Cross validation error of 10 different polynominal linear models:\n", cv.error.10)
glm.fit.best <- glm(log(rental_count) ~ type + month + weekday + hour +
humidity * dewpoint +
poly(windspeed, degree = best.level) +
poly(temperature, degree = best.level),
data=d.train)
# predict model on test data
glm.pred <- predict(glm.fit.best, newdata = d.test)
# convert log value to normal value via exponential
glm.pred <- exp(glm.pred)
# compute MSE
glm.mse <- mean((d.test$rental_count - glm.pred)^2)
cat("GLM polynomial model MSE: ", glm.mse)
# Chunk 17
# residual
resid_values <- resid(glm.fit.best)
cat("Residual length: ", length(resid_values))
cat("Residual header: ", head(resid_values))
# plot
par(mfrow=c(2,2))
plot(glm.fit.best)
# Chunk 18
# ggplot predicted values of hourly rental for different weather types
p1 <- ggplot(d.test, aes(x=hour, y=glm.pred, color=type)) +
geom_point(data=d.test, aes(group=type)) +
ggtitle("Prediction values") + ylab("predicted rentals / hour") +
scale_color_hue("Weather Type", breaks=levels(d.test$type))
# ggplot true values of hourly rental for different weather types
p2 <- ggplot(d.test, aes(x=hour, y=rental_count, color=type)) +
geom_point(data=d.test, aes(group=type)) +
ggtitle("Real values") + ylab("rentals / hour") +
scale_color_hue("Weather Type", breaks=levels(d.test$type))
figure <- ggarrange(p1, p2, ncol=2, common.legend = TRUE, legend="bottom")
annotate_figure(figure, top = text_grob("Hour & Weather on Rental - Polynomial Linear Model",
size = 14))
# Chunk 19
set.seed(1)
rf.fit <- randomForest(rental_count ~ . - rental_count - tripduration - date,
data = d.train, mtry=3, ntree=50, importance=TRUE)
rf.pred <- predict(rf.fit, d.test)
rf.mse <- mean((rf.pred - d.test$rental_count)^2)
cat("Randomforest mode MSE: ", rf.mse)
importance(rf.fit)
varImpPlot(rf.fit, main = "Randonforest Feature Importance")
# plot randomforest model fit
plot(rf.fit, main="Error rate vs. number of tree grown")
# ggplot predicted values of hourly rental for different weather types
p1 <- ggplot(d.test, aes(x=hour, y=rf.pred, color=type)) +
geom_point(data=d.test, aes(group=type)) +
ggtitle("Prediction values") + ylab("predicted rentals / hour") +
scale_color_hue("Weather Type", breaks=levels(d.test$type))
# ggplot true values of hourly rental for different weather types
p2 <- ggplot(d.test, aes(x=hour, y=rental_count, color=type)) +
geom_point(data=d.test, aes(group=type)) +
ggtitle("Real values") + ylab("rentals / hour") +
scale_color_hue("Weather Type", breaks=levels(d.test$type))
figure <- ggarrange(p1, p2, ncol=2, common.legend = TRUE, legend="bottom")
annotate_figure(figure, top = text_grob("Hour & Weather on Rental - Randomforest Model",
size = 14))
# Chunk 20
